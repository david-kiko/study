{"./":{"url":"./","title":"介绍","keywords":"","body":"学习汇总 GitBook 是一个基于 Node.js 的命令行工具，可使用 Github/Git 和 Markdown 来制作精美的电子书。 本书将简单介绍如何安装、编写、生成、发布一本在线图书 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-29 21:45:35 "},"bigdata/":{"url":"bigdata/","title":"大数据","keywords":"","body":"大数据 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-30 09:19:45 "},"bigdata/clickhouse/":{"url":"bigdata/clickhouse/","title":"clickhouse","keywords":"","body":"clickhouse © david all right reserved，powered by Gitbook文件修订时间： 2022-05-30 09:20:05 "},"bigdata/clickhouse/kafka_to_clickhouse.html":{"url":"bigdata/clickhouse/kafka_to_clickhouse.html","title":"Kafka引擎 数据导入","keywords":"","body":"ClickHouse-Kafka引擎，kafka to clickhouse详细教程 1.创建存储消费数据表 创建kafka_readings用于接收Kafka的数据 MergeTree 指定创建表的引擎 PARTITION BY 指定我们的分区数据 ORDER BY 指定我们的排序即索引 dt字段为具体业务时间。 CREATE TABLE kafka_readings ( source String, deviceId String, ts DateTime )Engine = MergeTree PARTITION BY toYYYYMMDD(ts) ORDER BY (ts); 2.创建消费Kafka数据表 kafka_broker_list kafka消费集群的broker列表 kafka_topic_list 消费kafka的Topic kafka_group_name kafka消费组 kafka_format 消费数据的格式化类型，JSONEachRow表示每行一条数据的json格式，如果需要输入嵌套的json，设置input_format_import_nested_json=1。更多：https://clickhouse.tech/docs/en/interfaces/formats/ kafka_skip_broken_messages 表示忽略解析异常的Kafka数据的条数。如果出现了N条异常后，后台线程结束，Materialized View会被重新安排后台线程去监听数据 kafka_num_consumers 单个Kafka Engine 的消费者数量，通过增加该参数，可以提高消费数据吞吐，但总数不应超过对应topic的partitions总数 CREATE TABLE kafka_readings_queue ( `message` String ) ENGINE = Kafka SETTINGS kafka_broker_list = 'testbee.cguardian.com:10012', kafka_topic_list = 'appupload', kafka_group_name = 'test_new_ck', kafka_format = 'JSONAsString', kafka_skip_broken_messages = 20000, kafka_num_consumers = 1; 3.创建物化视图合并表传输数据，导入到ClickHouse CREATE MATERIALIZED VIEW kafka_readings_view TO kafka_readings AS SELECT visitParamExtractRaw(message,'source') as source , visitParamExtractRaw(visitParamExtractRaw(message,'msg'),'deviceId') as deviceId , FROM_UNIXTIME(intDiv(visitParamExtractInt(JSONExtractString(visitParamExtractRaw(message,'msg'),'message',1),'start'),1000)) as ts FROM kafka_readings_queue; © david all right reserved，powered by Gitbook文件修订时间： 2022-05-30 09:24:12 "},"bigdata/clickhouse/refresh_kafka_to_clickhouse.html":{"url":"bigdata/clickhouse/refresh_kafka_to_clickhouse.html","title":"Kafka引擎 数据重新刷新","keywords":"","body":"ClickHouse-Kafka引擎，数据重新刷新，重新写回所有数据的操作步骤 1.首先关闭Kafka消息使用 DETACH TABLE kafka_readings_queue 2.清空Kafka数据存储表 truncate table kafka_readings 3.在Kafka主题的订阅使用者组中重置分区偏移量 --to-earliest：把位移调整到分区当前最小/早位移 --to-latest：把位移调整到分区当前最新位移 --to-current：把位移调整到分区当前位移 --to-offset ： 把位移调整到指定位移处 --shift-by N： 把位移调整到当前位移 + N处，注意N可以是负数，表示向前移动 --to-datetime ：把位移调整到大于给定时间的最早位移处，datetime格式是yyyy-MM-ddTHH:mm:ss.xxx，比如2017-08-04T00:00:00.000 kafka-consumer-groups.sh --bootstrap-server xxx --group test_new_ck --topic appupload -reset-offsets --to-earliest --dry-run kafka-consumer-groups.sh --bootstrap-server xxx --group test_new_ck --topic appupload -reset-offsets --to-earliest --execute 4.重新激活Kafka消息的使用 ATTACH TABLE kafka_readings_queue © david all right reserved，powered by Gitbook文件修订时间： 2022-05-30 09:31:05 "},"bigdata/flink/":{"url":"bigdata/flink/","title":"flink","keywords":"","body":"flink 相关内容 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-31 08:58:54 "},"bigdata/flink/flink-kafka-json.html":{"url":"bigdata/flink/flink-kafka-json.html","title":"对接kafka，处理json数据","keywords":"","body":"flink对接kafka，处理json数据 1.flink定义kafka metadata table CREATE TABLE `appupload`( source string, msg row, deviceProfile row, hostname string, isJailbroken boolean, is_new_inst boolean, kernBootTime timestamp, lac string, `language` string, mobileModel string, mobileType string, networkOperator string, osVersion string, ossdkVersion tinyint, pixelmetric string, pre_app_version string, simOperator string, timezone tinyint, wifiBssid string>, developerAppkey string, deviceId string, ip string, message array>, duration int, event array, sessionId string, id string, startTime timestamp>>, id string, mStatus tinyint, `start` bigint >>> >, ts as msg['message'][1]['session']['start'], ts_ltz AS TO_TIMESTAMP_LTZ(msg['message'][1]['session']['start'], 3), WATERMARK FOR ts_ltz AS ts_ltz - INTERVAL '10' SECOND ) WITH( 'connector' = 'kafka', 'topic' = 'appupload', 'properties.bootstrap.servers' = 'testbee.cguardian.com:10012', 'properties.group.id' = 'testGroup', 'format' = 'json', 'scan.startup.mode' = 'earliest-offset', 'json.fail-on-missing-field' = 'false', 'json.ignore-parse-errors' = 'true' ); 2.解析json数据 select day_str, source, count(1) FROM (SELECT distinct deviceId,source,FROM_UNIXTIME(msg['message'][1]['session']['start'] / 1000, 'yyyy-MM-dd') as day_str FROM ods_appupload) t group by day_str, source; 3.滑动窗口计算 SELECT window_start, window_end, source, count(1) FROM TABLE( HOP(TABLE appupload, DESCRIPTOR(ts_ltz), INTERVAL '5' MINUTES, INTERVAL '60' MINUTES)) GROUP BY window_start, window_end, source ; FlinkSQL窗口 https://www.modb.pro/db/78793 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-31 09:10:19 "},"bigdata/flink/flink-connector-mysql.html":{"url":"bigdata/flink/flink-connector-mysql.html","title":"对接mysql binlog","keywords":"","body":"flink-connector 对接mysql binlog 1.配置mysql source table CREATE TABLE `ybsj_xxb` ( `UID_SS` string, `CI_LASTUPDATE` TIMESTAMP(3) COMMENT '最后修改日期', `CI_LASTUPTIME` bigint COMMENT '最后修改时间', `CI_AUDITTAG` bigint COMMENT '审核标记', `DAY` string, `DIM_TBDW` string COMMENT '填报单位', `YBBH` string COMMENT '唯一键', `CYRQ` TIMESTAMP(3) COMMENT '采样日期', `XM` string COMMENT '姓名', `NL` bigint COMMENT '年龄', `XB` string COMMENT '性别', `SFZH` string COMMENT '身份证号', `JCORG` string COMMENT '检测机构', `BSRQ` TIMESTAMP(3) COMMENT '系统填报日期', `YBLY` string COMMENT '样本来源', `ZJLX` string COMMENT '证件类型', `SYBM` string COMMENT '送样编号', `BLLXJJCCS` string COMMENT '病例类型及检测次数', `HZID` string COMMENT '患者ID', `LXDH` string COMMENT '联系电话', `YBLX` string COMMENT '样本类型', `SFJYLH` string COMMENT '人员类型原：境外返（来）汉人员字段沿用下来', `qycj_rwdm` string COMMENT '全员检测任务代码', `qycj_rwmc` string COMMENT '全员检测任务名称', `CYDD` string COMMENT '采样地点', `HYFS` string COMMENT '混样方式', `sjly` string, `create_time` TIMESTAMP(0), `sz_uptime` TIMESTAMP(0), `ssrq` string, `cyfzr` string, `lxfs` string, `data_up_uuid` int, `data_up_time` TIMESTAMP(0), `data_up_status` string, PRIMARY KEY (data_up_uuid) NOT ENFORCED ) WITH ( 'connector' = 'mysql-cdc', 'hostname' = '116.1.10.1', 'port' = '20783', 'username' = 'left_flink_user', 'password' = 'left_flink_user', 'database-name' = 'left_zsj', 'table-name' = 'ybsj_xxb', 'server-time-zone' = 'Asia/Shanghai', 'scan.startup.mode' = 'latest-offset', 'scan.snapshot.fetch.size' = '2048', 'debezium.skipped.operations'='u,d' ); 2.创建sink table CREATE TABLE `ybsj_xxb_s` ( `UID_SS` string, `CI_LASTUPDATE` TIMESTAMP(3) COMMENT '最后修改日期', `CI_LASTUPTIME` bigint COMMENT '最后修改时间', `CI_AUDITTAG` bigint COMMENT '审核标记', `DAY` string, `DIM_TBDW` string COMMENT '填报单位', `YBBH` string COMMENT '唯一键', `CYRQ` TIMESTAMP(3) COMMENT '采样日期', `XM` string COMMENT '姓名', `NL` bigint COMMENT '年龄', `XB` string COMMENT '性别', `SFZH` string COMMENT '身份证号', `JCORG` string COMMENT '检测机构', `BSRQ` TIMESTAMP COMMENT '系统填报日期', `YBLY` string COMMENT '样本来源', `ZJLX` string COMMENT '证件类型', `SYBM` string COMMENT '送样编号', `BLLXJJCCS` string COMMENT '病例类型及检测次数', `HZID` string COMMENT '患者ID', `LXDH` string COMMENT '联系电话', `YBLX` string COMMENT '样本类型', `SFJYLH` string COMMENT '人员类型原：境外返（来）汉人员字段沿用下来', `qycj_rwdm` string COMMENT '全员检测任务代码', `qycj_rwmc` string COMMENT '全员检测任务名称', `CYDD` string COMMENT '采样地点', `HYFS` string COMMENT '混样方式', `sjly` string, `create_time` TIMESTAMP, `sz_uptime` TIMESTAMP, `ssrq` string, `cyfzr` string, `lxfs` string, `data_up_uuid` int, `data_up_time` TIMESTAMP(0), PRIMARY KEY (data_up_uuid) NOT ENFORCED ) WITH ( 'connector' = 'jdbc', 'url' = 'jdbc:mysql://116.1.20.65:13307/wjw?serverTimezone=Asia/Shanghai&useunicode=true&characterEncoding=utf8&yearIsDateType=false&zeroDateTimeBehavior=convertToNull&tinyInt1isBit=false&rewriteBatchedStatements=true', 'username' = 'wjw', 'password' = 'Xfn33edfgt4KANY', 'table-name' = 'ybsj_xxb_s', 'sink.buffer-flush.max-rows' = '2048', 'sink.parallelism' = '12', 'sink.buffer-flush.interval' = '2s' ); 3.从source导入数据到sink insert into ybsj_xxb_s(`UID_SS`,`CI_LASTUPDATE`,`CI_LASTUPTIME`,`CI_AUDITTAG`,`DAY`,`DIM_TBDW`,`YBBH`,`CYRQ`,`XM`,`NL`,`XB`,`SFZH`, `JCORG`,`BSRQ`,`YBLY`,`ZJLX`,`SYBM` ,`BLLXJJCCS`,`HZID`,`LXDH`,`YBLX`,`SFJYLH`,`qycj_rwdm`,`qycj_rwmc`,`CYDD`, `HYFS`,`sjly`,`create_time`,`sz_uptime`,`ssrq`,`cyfzr`,`lxfs`,`data_up_uuid`,`data_up_time`) select `UID_SS`,`CI_LASTUPDATE`,`CI_LASTUPTIME`,`CI_AUDITTAG`,`DAY`,`DIM_TBDW`,`YBBH`,`CYRQ`,`XM`,`NL`,`XB`,`SFZH`, `JCORG`,`BSRQ`,`YBLY`,`ZJLX`,`SYBM` ,`BLLXJJCCS`,`HZID`,`LXDH`,`YBLX`,`SFJYLH`,`qycj_rwdm`,`qycj_rwmc`,`CYDD`, `HYFS`,`sjly`,`create_time`,`sz_uptime`,`ssrq`,`cyfzr`,`lxfs`,`data_up_uuid`,`data_up_time` from ybsj_xxb; © david all right reserved，powered by Gitbook文件修订时间： 2022-05-31 09:31:36 "},"deploy/":{"url":"deploy/","title":"快速部署","keywords":"","body":"组件安装步骤 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-30 10:18:30 "},"deploy/k8s/":{"url":"deploy/k8s/","title":"k8s","keywords":"","body":"k8s相关安装步骤 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-30 10:18:48 "},"deploy/k8s/k8s-kubeadm.html":{"url":"deploy/k8s/k8s-kubeadm.html","title":"k8s kubeadm安装","keywords":"","body":"k8s v.1.21集群安装部署过程（kubeadm方式） 一、设置主机名 hostnamectl set-hostname k8s-master hostnamectl set-hostname k8s-node01 hostnamectl set-hostname k8s-node01 二、升级内核 1.安装内核 wget https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/k8s-install/kernel-lt-5.4.114-1.el7.elrepo.x86_64.rpm wget https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/k8s-install/kernel-lt-devel-5.4.114-1.el7.elrepo.x86_64.rpm yum -y install kernel-lt-5.4.114-1.el7.elrepo.x86_64.rpm kernel-lt-devel-5.4.114-1.el7.elrepo.x86_64.rpm 2.调整默认内核启动 grub2-set-default \"CentOS Linux (5.4.114-1.el7.elrepo.x86_64) 7 (Core)\" 3.检查是否修改正确并重启系统 grub2-editenv list reboot 三、将桥接的IPv4流量传递到iptables的链 modprobe br_netfilter cat > /etc/sysctl.d/k8s.conf 四、开启IPVS支持 创建/etc/sysconfig/modules/ipvs.modules文件，内容如下： #!/bin/bash ipvs_modules=\"ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack\" for kernel_module in ${ipvs_modules}; do /sbin/modinfo -F filename ${kernel_module} > /dev/null 2>&1 if [ $? -eq 0 ]; then /sbin/modprobe ${kernel_module} fi done 最后，执行如下命令使配置生效： chmod 755 /etc/sysconfig/modules/ipvs.modules sh /etc/sysconfig/modules/ipvs.modules lsmod | grep ip_vs 下载ipvs管理工具 yum install -y ipset ipvsadm 五、同步服务器时间 yum install chrony -y systemctl enable chronyd --now chronyc sources 六、关闭防火墙、selinux 1.K8s集群每个节点都需要关闭防火墙，执行如下操作： systemctl stop firewalld && systemctl disable firewalld setenforce 0 sed -i \"s/SELINUX=enforcing/SELINUX=disabled/g\" /etc/selinux/config 2.接着，还需要关闭系统的交换分区，执行如下命令： swapoff -a cp /etc/fstab /etc/fstab.bak cat /etc/fstab.bak | grep -v swap > /etc/fstab 3.然后，还需要修改iptables设置，在/etc/sysctl.conf中添加如下内容： cat >> /etc/sysctl.conf 4.最后，执行如下命令，以使设置生效： sysctl -p 七、主机名本地解析配置 每个主机的主机名以及IP地址都在上面环境介绍中给出来了，根据这些信息，在每个k8s集群节点添加如下主机名解析信息，将这些信息添加到每个集群节点的/etc/hosts文件中，主机名解析内容如下： 127.0.1.1 i-jg8xmnmc 192.168.0.21 k8s-master 192.168.0.22 k8s-node01 192.168.0.23 k8s-node02 注意：使用内网地址 八、安装docker环境（所有节点都执行下面步骤） 1.安装 yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo yum makecache fast yum install docker-ce -y systemctl restart docker systemctl enable docker docker version 2.配置 cat 3.重新启动 Docker 并在启动时启用： systemctl enable docker systemctl daemon-reload systemctl restart docker 九、安装Containerd 1.下载并配置 cd /root/ yum install libseccomp -y wget https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/k8s-install/cri-containerd-cni-1.5.5-linux-amd64.tar.gz tar -C / -xzf cri-containerd-cni-1.5.5-linux-amd64.tar.gz echo \"export PATH=$PATH:/usr/local/bin:/usr/local/sbin\" >> ~/.bashrc source ~/.bashrc mkdir -p /etc/containerd containerd config default > /etc/containerd/config.toml systemctl enable containerd --now ctr version 2.将 containerd 的 cgroup driver 配置为 systemd #通过搜索SystemdCgroup进行定位 vim /etc/containerd/config.toml [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc] ... [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.runc.options] SystemdCgroup = true .... #注意：最终输出shell命令： sed -i \"s/SystemdCgroup = false/SystemdCgroup = true/g\" /etc/containerd/config.toml 3.配置镜像加速器地址 然后再为镜像仓库配置一个加速器，需要在 cri 配置块下面的 registry 配置块下面进行配置 registry.mirrors：（注意缩进） [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"docker.io\"] endpoint = [\"https://kvuwuws2.mirror.aliyuncs.com\"] [plugins.\"io.containerd.grpc.v1.cri\".registry.mirrors.\"k8s.gcr.io\"] endpoint = [\"https://registry.aliyuncs.com/k8sxio\"] 4.启动containerd服务 systemctl daemon-reload systemctl enable containerd --now 5.验证 ctr version crictl version 十、安装Kubeadm、kubelet、kubectl工具 1.在确保系统基础环境配置完成后，现在我们就可以来安装 Kubeadm、kubelet了，我这里是通过指定yum 源的方式来进行安装，使用阿里云的源进行安装： cat > /etc/yum.repos.d/kubernetes.repo 2.安装 kubeadm、kubelet、kubectl(all节点均要配置)。 yum makecache fast yum install -y kubelet-1.22.2 kubeadm-1.22.2 kubectl-1.22.2 --disableexcludes=kubernetes kubeadm version systemctl enable --now kubelet kubeadm version 十一、初始化集群(master节点操作) 1.执行如下命令: kubeadm config print init-defaults kubeadm config images list kubeadm config images list --kubernetes-version=v1.21.0 --image-repository swr.myhuaweicloud.com/iivey 其中： 第一个命令用来查看安装k8s的相关信息，主要是安装源和版本。 第二条命令是查询需要的镜像，从输出可在，默认是从k8s.gcr.io这个地址下载镜像，此地址国内无法访问，因此需要修改默认下载镜像的地址。 第三条命令是设置k8s镜像仓库为华为云镜像站地址，查看一下需要下载的镜像都有哪些。 2.接着，在 master 节点配置 kubeadm 初始化文件，可以通过如下命令导出默认的初始化配置： kubeadm config print init-defaults --component-configs KubeletConfiguration > kubeadm.yaml 3.然后根据我们自己的需求修改配置，比如修改 imageRepository 指定集群初始化时拉取 Kubernetes 所需镜像的地址，kube-proxy 的模式为 ipvs，另外需要注意的是我们这里是准备安装 flannel 网络插件的，需要将 networking.podSubnet 设置为10.244.0.0/16。 apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.0.21 # 修改1：指定master节点内网IP bindPort: 6443 nodeRegistration: criSocket: /run/containerd/containerd.sock # 修改2：使用 containerd的Unix socket 地址 imagePullPolicy: IfNotPresent name: k8s-master01 #name #修改3：修改master节点名称 taints: # 修改4：给master添加污点，master节点不能调度应用 - effect: \"NoSchedule\" key: \"node-role.kubernetes.io/master\" --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs # 修改5：修改kube-proxy 模式为ipvs，默认为iptables --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: registry.aliyuncs.com/k8sxio #修改6：image地址 kind: ClusterConfiguration kubernetesVersion: 1.22.2 #修改7：指定k8s版本号，默认这里忽略了小版本号 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 # 修改8：指定 pod 子网 scheduler: {} --- apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: cacheTTL: 0s enabled: true x509: clientCAFile: /etc/kubernetes/pki/ca.crt authorization: mode: Webhook webhook: cacheAuthorizedTTL: 0s cacheUnauthorizedTTL: 0s clusterDNS: - 10.96.0.10 clusterDomain: cluster.local cpuManagerReconcilePeriod: 0s evictionPressureTransitionPeriod: 0s fileCheckFrequency: 0s healthzBindAddress: 127.0.0.1 healthzPort: 10248 httpCheckFrequency: 0s imageMinimumGCAge: 0s kind: KubeletConfiguration cgroupDriver: systemd # 修改9：配置 cgroup driver logging: {} memorySwap: {} nodeStatusReportFrequency: 0s nodeStatusUpdateFrequency: 0s rotateCertificates: true runtimeRequestTimeout: 0s shutdownGracePeriod: 0s shutdownGracePeriodCriticalPods: 0s staticPodPath: /etc/kubernetes/manifests streamingConnectionIdleTimeout: 0s syncFrequency: 0s volumeStatsAggPeriod: 0s 4.提前下载镜像 kubeadm config images list kubeadm config images list --config kubeadm.yaml kubeadm config images pull --config kubeadm.yaml 5.上面在拉取 coredns 镜像的时候出错了，阿里云仓库里没有找到这个镜像，我们可以手动到官方仓库 pull 该镜像，然后重新 tag 下镜像地址即可 ctr -n k8s.io i pull docker.io/coredns/coredns:1.8.4 ctr -n k8s.io i tag docker.io/coredns/coredns:1.8.4 registry.aliyuncs.com/k8sxio/coredns:v1.8.4 6.pause镜像会使用k8s.gcr.io镜像仓库，提前打上tag ctr -n k8s.io i tag registry.aliyuncs.com/k8sxio/pause:3.5 k8s.gcr.io/pause:3.5 ctr -n k8s.io i ls -q 或者修改/etc/containerd/config.toml文件，sandbox_image字段 7.初始化master节点 kubeadm init --config kubeadm.yaml 8.根据安装提示拷贝 kubeconfig 文件 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config 9.查看安装效果 kubectl get node 十二、添加节点 kubeadm join 192.168.0.21:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:ad45d9d60ddab962d72c054de05f41e130b4cf42a75d8e97e96b63331cf9fbf1 join 命令：如果忘记了上面的 join 命令可以使用命令 kubeadm token create --print-join-command 重新获取。 十三、安装网络插件flannel 这个时候其实集群还不能正常使用，因为还没有安装网络插件，接下来安装网络插件，可以在文档 https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ 中选择我们自己的网络插件，这里我们安装 flannel: wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 这个文件已经配置好，直接apply: kubectl apply -f kube-flannel.yml 隔一会儿查看 Pod 运行状态: kubectl get pod -nkube-system -owide 十四、Dashboard 1.下载kube-dashboard的yaml文件 wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 2.文件内容 # Copyright 2017 The Kubernetes Authors. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. apiVersion: v1 kind: Namespace metadata: name: kubernetes-dashboard --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-certs namespace: kubernetes-dashboard type: Opaque --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-csrf namespace: kubernetes-dashboard type: Opaque data: csrf: \"\" --- apiVersion: v1 kind: Secret metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-key-holder namespace: kubernetes-dashboard type: Opaque --- kind: ConfigMap apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-settings namespace: kubernetes-dashboard --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard rules: # Allow Dashboard to get, update and delete Dashboard exclusive secrets. - apiGroups: [\"\"] resources: [\"secrets\"] resourceNames: [\"kubernetes-dashboard-key-holder\", \"kubernetes-dashboard-certs\", \"kubernetes-dashboard-csrf\"] verbs: [\"get\", \"update\", \"delete\"] # Allow Dashboard to get and update 'kubernetes-dashboard-settings' config map. - apiGroups: [\"\"] resources: [\"configmaps\"] resourceNames: [\"kubernetes-dashboard-settings\"] verbs: [\"get\", \"update\"] # Allow Dashboard to get metrics. - apiGroups: [\"\"] resources: [\"services\"] resourceNames: [\"heapster\", \"dashboard-metrics-scraper\"] verbs: [\"proxy\"] - apiGroups: [\"\"] resources: [\"services/proxy\"] resourceNames: [\"heapster\", \"http:heapster:\", \"https:heapster:\", \"dashboard-metrics-scraper\", \"http:dashboard-metrics-scraper\"] verbs: [\"get\"] --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard rules: # Allow Metrics Scraper to get metrics from the Metrics server - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kubernetes-dashboard subjects: - kind: ServiceAccount name: kubernetes-dashboard namespace: kubernetes-dashboard --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: kubernetes-dashboard template: metadata: labels: k8s-app: kubernetes-dashboard spec: containers: - name: kubernetes-dashboard image: kubernetesui/dashboard:v2.3.1 imagePullPolicy: Always ports: - containerPort: 8443 protocol: TCP args: - --auto-generate-certificates - --namespace=kubernetes-dashboard # Uncomment the following line to manually specify Kubernetes API server Host # If not specified, Dashboard will attempt to auto discover the API server and connect # to it. Uncomment only if the default does not work. # - --apiserver-host=http://my-address:port volumeMounts: - name: kubernetes-dashboard-certs mountPath: /certs # Create on-disk volume to store exec logs - mountPath: /tmp name: tmp-volume livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 initialDelaySeconds: 30 timeoutSeconds: 30 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 volumes: - name: kubernetes-dashboard-certs secret: secretName: kubernetes-dashboard-certs - name: tmp-volume emptyDir: {} serviceAccountName: kubernetes-dashboard nodeSelector: \"kubernetes.io/os\": linux # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule --- kind: Service apiVersion: v1 metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboard spec: ports: - port: 8000 targetPort: 8000 selector: k8s-app: dashboard-metrics-scraper --- kind: Deployment apiVersion: apps/v1 metadata: labels: k8s-app: dashboard-metrics-scraper name: dashboard-metrics-scraper namespace: kubernetes-dashboard spec: replicas: 1 revisionHistoryLimit: 10 selector: matchLabels: k8s-app: dashboard-metrics-scraper template: metadata: labels: k8s-app: dashboard-metrics-scraper annotations: seccomp.security.alpha.kubernetes.io/pod: 'runtime/default' spec: containers: - name: dashboard-metrics-scraper image: kubernetesui/metrics-scraper:v1.0.6 ports: - containerPort: 8000 protocol: TCP livenessProbe: httpGet: scheme: HTTP path: / port: 8000 initialDelaySeconds: 30 timeoutSeconds: 30 volumeMounts: - mountPath: /tmp name: tmp-volume securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true runAsUser: 1001 runAsGroup: 2001 serviceAccountName: kubernetes-dashboard nodeSelector: \"kubernetes.io/os\": linux # Comment the following tolerations if Dashboard must not be deployed on master tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: tmp-volume emptyDir: {} 3.修改kube-dashboard.yaml文件 # 修改Service为NodePort类型 ...... kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: ports: - port: 443 targetPort: 8443 selector: k8s-app: kubernetes-dashboard type: NodePort # 加上type=NodePort变成NodePort类型的服务 ...... 4.部署kube-dashboard.yaml文件 mv recommended.yaml kube-dashboard.yaml kubectl apply -f kube-dashboard.yaml 十五、配置cni 我们仔细看可以发现上面的 Pod 分配的 IP 段是 10.88.xx.xx，包括前面自动安装的 CoreDNS 也是如此，我们前面不是配置的 podSubnet 为 10.244.0.0/16 吗？ kubectl get pod -A -owide 我们先去查看下 CNI 的配置文件 ll /etc/cni/net.d/ 可以看到里面包含两个配置，一个是 10-containerd-net.conflist，另外一个是我们上面创建的 Flannel 网络插件生成的配置，我们的需求肯定是想使用 Flannel 的这个配置，我们可以查看下 containerd 这个自带的 cni 插件配置： cat /etc/cni/net.d/10-containerd-net.conflist { \"cniVersion\": \"0.4.0\", \"name\": \"containerd-net\", \"plugins\": [ { \"type\": \"bridge\", \"bridge\": \"cni0\", \"isGateway\": true, \"ipMasq\": true, \"promiscMode\": true, \"ipam\": { \"type\": \"host-local\", \"ranges\": [ [{ \"subnet\": \"10.88.0.0/16\" }], [{ \"subnet\": \"2001:4860:4860::/64\" }] ], \"routes\": [ { \"dst\": \"0.0.0.0/0\" }, { \"dst\": \"::/0\" } ] } }, { \"type\": \"portmap\", \"capabilities\": {\"portMappings\": true} } ] } 可以看到上面的 IP 段恰好就是 10.88.0.0/16，但是这个 cni 插件类型是 bridge 网络，网桥的名称为 cni0： 但是使用 bridge 网络的容器无法跨多个宿主机进行通信，跨主机通信需要借助其他的 cni 插件，比如上面我们安装的 Flannel，或者 Calico 等等，由于我们这里有两个 cni 配置，所以我们需要将 10-containerd-net.conflist 这个配置删除，因为如果这个目录中有多个 cni 配置文件，kubelet 将会使用按文件名的字典顺序排列的第一个作为配置文件，所以前面默认选择使用的是 containerd-net 这个插件。 mv /etc/cni/net.d/10-containerd-net.conflist{,.bak} ifconfig cni0 down && ip link delete cni0 systemctl daemon-reload systemctl restart containerd kubelet 然后记得重建 coredns 和 dashboard 的 Pod，重建后 Pod 的 IP 地址就正常了： kubectl delete pod 十六、创建一个具有全局所有权限的用户来登录 Dashboard：(admin.yaml) vim admin.yaml kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: admin roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: admin namespace: kubernetes-dashboard --- apiVersion: v1 kind: ServiceAccount metadata: name: admin namespace: kubernetes-dashboard 直接创建 kubectl apply -f admin.yaml kubectl get secret admin-token-xxxxx -o jsonpath={.data.token} -n kubernetes-dashboard |base64 -d 然后用上面的 base64 解码后的字符串作为 token 登录 Dashboard 即可 十七、修改node节点container-runtime 执行kubectl get nodes -o wide命令，可以看的node01,node02还是使用docker作为运行时容器 1.维护节点 首先将需要修改的节点设置成不可调度 kubectl cordon k8s-node02 kubectl get nodes -o wide 驱逐该节点上除了daemonset的pod kubectl drain k8s-node02 --ignore-daemonsets 2.切换Runtime 关闭docker、containerd 和 kubelet： systemctl stop kubelet systemctl stop docker systemctl stop containerd 3.安装containerd 我们在使用docker-ce作为集群runtime时默认安装了containerd，先将其卸载。 yum remove docker-ce docker-ce-cli containerd.io 安装containerd可参考上面操作 /etc/containerd/config.toml 修改默认的 pause 镜像为国内的地址，替换 [plugins.\"io.containerd.grpc.v1.cri\"] 下面的 sandbox_image 配置下镜像仓库的加速器地址 4.配置kubelet 修改 kubelet 配置，将容器运行时配置为 containerd，编辑/etc/sysconfig/kubelet 文件，在该文件中可以添加kubelet 启动参数： KUBELET_EXTRA_ARGS=\"--container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock\" --container-runtime ：指定使用的容器运行时的，可选值为 docker 或者 remote，默认是 docker，除 docker 之外的容器运行时都应该指定为 remote。 --container-runtime-endpoint：是用来指定远程的运行时服务的 endpiont 地址的，在 Linux 系统中一般都是使用 unix 套接字的形式，unix:///run/containerd/containerd.sock。 --image-service-endpoint：指定远程 CRI 的镜像服务地址，如果没有指定则默认使用 --container-runtime-endpoint 的值了，因为 CRI 都会实现容器和镜像服务的。 5.配置完成后重启 containerd 和 kubelet 即可 systemctl daemon-reload systemctl restart containerd systemctl restart kubelet 6.查看服务 crictl -v kubectl get nodes -o wide 7.运行调度 kubectl uncordon k8s-node02 十八、参考地址 https://mdnice.com/writing/3e3ec25bfa464049ae173c31a6d98cf8 https://www.jianshu.com/p/5f1630f3bd63 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-30 10:20:07 "},"deploy/clickhouse/":{"url":"deploy/clickhouse/","title":"clickhouse","keywords":"","body":"clickhouse安装步骤 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-30 10:31:39 "},"deploy/clickhouse/clickhouse-docker.html":{"url":"deploy/clickhouse/clickhouse-docker.html","title":"容器部署clickhouse + tabix","keywords":"","body":"clickhouse docker安装 一、搭建clcikhouse 1.docker-compose.yml version: \"3.6\" services: clickhouse-halobug: container_name: clickhouse-halobug.cn image: yandex/clickhouse-server:latest restart: always ports: - \"8123:8123\" - \"9000:9000\" - \"9009:9009\" networks: - traefik volumes: # 默认配置 写入config.d/users.d 目录防止更新后文件丢失 - ./config.xml:/etc/clickhouse-server/config.d/config.xml:rw - ./users.xml:/etc/clickhouse-server/users.xml:rw # 运行日志 - ./logs:/var/log/clickhouse-server # 数据持久 - ./data:/var/lib/clickhouse:rw networks: traefik: external: true 2.配置文件内容生成 启动时先不挂载config.xml以及users.xml docker cp 容器名:/etc/clickhouse-server/config.xml . docker cp 容器名:/etc/clickhouse-server/uses.xml . 3.创建用户登录 修改users.xml文件，自定义用户 4754fc7e290a9c280d9497b2d76dd854e77f7e1c92476577fdb52ed22afc13e7 ::/0 default default // 自定义对应数据库 tutorial 为测试库 tutorial 生成密码（进入容器运行） PASSWORD=$(base64 4.验证用户密码，重新启动服务 # 重启 docker-compose down && docker-compose up -d # 进入容器 docker exec -it clickhouse-halobug.cn /bin/bash # 验证用户名密码 clickhouse-client -u halobug -h 127.0.0.1 --password qjLlan5A 二、搭建tabix可视化操作 1.tabix docker-compose.yml # docker-compose.yml version: \"3.6\" services: clickhouse-halobug: container_name: tabix-halobug.cn image: spoonest/clickhouse-tabix-web-client restart: always expose: - 80 networks: - traefik labels: - \"traefik.enable=true\" - \"traefik.docker.network=traefik\" - \"traefik.http.routers.tabix-test.entrypoints=https\" - \"traefik.http.routers.tabix-test.rule=Host(`tabix.halobug.cn`)\" - \"traefik.http.routers.tabix-test.tls=true\" - \"traefik.http.services.tabix-test-backend.loadbalancer.server.scheme=http\" - \"traefik.http.services.tabix-test-backend.loadbalancer.server.port=80\" logging: driver: \"json-file\" options: max-size: \"1m\" networks: traefik: external: true 2.访问https://127.0.0.1 使用上面创建的用户及密码登录 三、测试命令 # 进入容器 docker exec -it clickhouse-halobug.cn /bin/bash # 创建数据库 clickhouse-client --query \"CREATE DATABASE IF NOT EXISTS test\" #使用用户名进入 clickhouse-client -u halobug -h 127.0.0.1 --password qjLlan5A # 创建测试表 CREATE TABLE test.test_son ( `id` Int64, `name` String, `title` String ) ENGINE = MergeTree ORDER BY id SETTINGS index_granularity = 8192 # 插入数据 INSERT INTO test.test_son VALUES ('1','Hello, world','标题测试'), ('2','Hello, world2','标题测试3') 四、参考文档 https://zhuanlan.zhihu.com/p/383817560 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-30 11:05:09 "},"deploy/flink/":{"url":"deploy/flink/","title":"flink","keywords":"","body":"© david all right reserved，powered by Gitbook文件修订时间： 2022-05-31 08:18:11 "},"deploy/flink/flink-ha-systemd.html":{"url":"deploy/flink/flink-ha-systemd.html","title":"flink ha 部署","keywords":"","body":"flink ha systemd部署 1.下载解压flink包 wget https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/realtime/flink-1.13.2-bin-scala_2.11.tgz tar -xzvf flink-1.13.2-bin-scala_2.11.tgz mv flink-1.13.2 /etc/flink 2.下载connector及hadoop相关包 wget -O lib/flink-sql-connector-elasticsearch7_2.11-1.13.2.jar https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/realtime/flink-sql-connector-elasticsearch7_2.11-1.13.2.jar wget -O lib/flink-sql-connector-mongodb-cdc-2.1.0.jar https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/realtime/flink-sql-connector-mongodb-cdc-2.1.0.jar wget -O lib/flink-sql-connector-mysql-cdc-2.1.0.jar https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/realtime/flink-sql-connector-mysql-cdc-2.1.0.jar wget -O lib/flink-sql-connector-oracle-cdc-2.1.1.jar https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/realtime/flink-sql-connector-oracle-cdc-2.1.1.jar wget -O lib/flink-sql-connector-postgres-cdc-2.1.0.jar https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/realtime/flink-sql-connector-postgres-cdc-2.1.0.jar wget -O lib/flink-connector-jdbc_2.11-1.13.2.jar https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/realtime/flink-connector-jdbc_2.11-1.13.2.jar wget -O lib/flink-shaded-hadoop-2-uber-2.7.5-10.0.jar https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/realtime/flink-shaded-hadoop-2-uber-2.7.5-10.0.jar 3.配置flink conf security.kerberos.login.use-ticket-cache: false security.kerberos.login.keytab: /home/dcos/dcos.keytab security.kerberos.login.principal: dcos@LINKTIME.CLOUD env.java.opts: -Djava.security.krb5.conf=/etc/krb5.conf state.backend: filesystem state.checkpoints.dir: hdfs://default/flink/flink-checkpoints state.savepoints.dir: hdfs://default/flink/flink-checkpoints state.backend.incremental: false high-availability: zookeeper high-availability.zookeeper.quorum: zk-1.zk:2181,zk-2.zk:2181,zk-3.zk:2181,zk-4.zk:2181,zk-5.zk:2181 high-availability.storageDir: hdfs://default/flink/ha high-availability.zookeeper.path.root: /flink high-availability.cluster-id: /flink_cluster jobmanager.archive.fs.dir: hdfs://default/flink/completed-jobs/ historyserver.archive.fs.dir: hdfs://default/flink/completed-jobs/ classloader.resolve-order: parent-first restart-strategy: fixed-delay restart-strategy.fixed-delay.delay: 60 s cluster.evenly-spread-out-slots: true 4.配置hadoop路径 config.sh文件中添加export语句 export HADOOP_CONF_DIR=/etc/hadoop/etc/hadoop 5.设置systemd服务 jobmanager master cat > /usr/lib/systemd/system/flink-jobmanager.service taskmanager agent cat > /usr/lib/systemd/system/flink-taskmanager.service 6.加载并设置开机启动 systemctl daemon-reload systemctl start flink-taskmanager.service systemctl enable flink-taskmanager.service © david all right reserved，powered by Gitbook文件修订时间： 2022-05-31 08:34:48 "},"deploy/flink/flink-web.html":{"url":"deploy/flink/flink-web.html","title":"flink web 部署","keywords":"","body":"flink web部署 1.下载解压 wget https://linktime-external-project.oss-cn-hangzhou.aliyuncs.com/realtime/flink-streaming-platform-web.tar.gz tar -zxvf flink-streaming-platform-web.tar.gz 2.mysql中执行sql SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; CREATE DATABASE `flink_web` DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; USE flink_web; -- ---------------------------- -- Table structure for alart_log -- ---------------------------- DROP TABLE IF EXISTS `alart_log`; CREATE TABLE `alart_log` ( `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT, `job_config_id` bigint(11) NOT NULL COMMENT 'job_config的id 如果0代表的是测试,', `job_name` varchar(255) DEFAULT NULL, `message` varchar(512) DEFAULT NULL COMMENT '消息内容', `type` tinyint(1) NOT NULL COMMENT '1:钉钉', `status` tinyint(1) NOT NULL COMMENT '1:成功 0:失败', `fail_log` text COMMENT '失败原因', `is_deleted` tinyint(1) NOT NULL DEFAULT '0', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `creator` varchar(32) DEFAULT 'sys', `editor` varchar(32) DEFAULT 'sys', PRIMARY KEY (`id`), KEY `index_job_config_id` (`job_config_id`) USING BTREE ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='告警发送情况日志'; -- ---------------------------- -- Table structure for ip_status -- ---------------------------- DROP TABLE IF EXISTS `ip_status`; CREATE TABLE `ip_status` ( `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT, `ip` varchar(64) NOT NULL COMMENT 'ip', `status` int(11) NOT NULL COMMENT '1:运行 -1:停止 ', `last_time` datetime DEFAULT NULL COMMENT '最后心跳时间', `is_deleted` tinyint(1) NOT NULL DEFAULT '0', `create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `edit_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `creator` varchar(32) NOT NULL DEFAULT 'sys', `editor` varchar(32) NOT NULL DEFAULT 'sys', PRIMARY KEY (`id`), UNIQUE KEY `index_uk_ip` (`ip`) USING BTREE ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='web服务运行ip'; -- ---------------------------- -- Table structure for job_config -- ---------------------------- DROP TABLE IF EXISTS `job_config`; CREATE TABLE `job_config` ( `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT, `job_name` varchar(64) NOT NULL COMMENT '任务名称', `deploy_mode` varchar(64) NOT NULL COMMENT '提交模式: standalone 、yarn 、yarn-session ', `flink_run_config` varchar(512) NOT NULL COMMENT 'flink运行配置', `flink_sql` MEDIUMTEXT NOT NULL COMMENT 'sql语句', `flink_checkpoint_config` varchar(512) DEFAULT NULL COMMENT 'checkPoint配置', `job_id` varchar(64) DEFAULT NULL COMMENT '运行后的任务id', `is_open` tinyint(1) NOT NULL COMMENT '1:开启 0: 关闭', `status` tinyint(1) NOT NULL COMMENT '1:运行中 0: 停止中 -1:运行失败', `ext_jar_path` varchar(2048) DEFAULT NULL COMMENT 'udf地址已经连接器jar 如http://xxx.xxx.com/flink-streaming-udf.jar', `last_start_time` datetime DEFAULT NULL COMMENT '最后一次启动时间', `last_run_log_id` bigint(11) DEFAULT NULL COMMENT '最后一次日志', `version` int(11) NOT NULL DEFAULT '0' COMMENT '更新版本号 用于乐观锁', `is_deleted` tinyint(1) NOT NULL DEFAULT '0', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `creator` varchar(32) DEFAULT 'sys', `editor` varchar(32) DEFAULT 'sys', PRIMARY KEY (`id`), KEY `uk_index` (`job_name`) USING BTREE ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='flink任务配置表'; ALTER TABLE job_config add `job_type` tinyint(1) NOT NULL DEFAULT '0' COMMENT '任务类型 0:sql 1:自定义jar' AFTER version ; ALTER TABLE job_config add `custom_args` varchar(128) DEFAULT NULL COMMENT '启动jar可能需要使用的自定义参数' AFTER job_type; ALTER TABLE job_config add `custom_main_class` varchar(128) DEFAULT NULL COMMENT '程序入口类' AFTER custom_args; ALTER TABLE job_config add `custom_jar_url` varchar(128) DEFAULT NULL COMMENT'自定义jar的http地址 如:http://ccblog.cn/xx.jar' AFTER custom_main_class; ALTER TABLE job_config ADD COLUMN job_desc VARCHAR(255) NULL COMMENT '任务描述' AFTER job_name; -- ---------------------------- -- Table structure for job_config_history -- ---------------------------- CREATE TABLE `job_config_history` ( `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT, `job_config_id` bigint(11) NOT NULL COMMENT 'job_config主表Id', `job_name` varchar(64) NOT NULL COMMENT '任务名称', `deploy_mode` varchar(64) NOT NULL COMMENT '提交模式: standalone 、yarn 、yarn-session ', `flink_run_config` varchar(512) NOT NULL COMMENT 'flink运行配置', `flink_sql` mediumtext NOT NULL COMMENT 'sql语句', `flink_checkpoint_config` varchar(512) DEFAULT NULL COMMENT 'checkPoint配置', `ext_jar_path` varchar(2048) DEFAULT NULL COMMENT 'udf地址及连接器jar 如http://xxx.xxx.com/flink-streaming-udf.jar', `version` int(11) NOT NULL DEFAULT '0' COMMENT '更新版本号', `is_deleted` tinyint(1) NOT NULL DEFAULT '0', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `creator` varchar(32) DEFAULT 'sys', `editor` varchar(32) DEFAULT 'sys', PRIMARY KEY (`id`), KEY `index_job_config_id` (`job_config_id`) USING BTREE ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='flink任务配置历史变更表'; ALTER TABLE job_config_history ADD COLUMN job_desc VARCHAR(255) NULL COMMENT '任务描述' AFTER job_name; ALTER TABLE job_config_history add `job_type` tinyint(1) NOT NULL DEFAULT '0' COMMENT '任务类型 0:sql 1:自定义jar' AFTER version ; -- ---------------------------- -- Table structure for job_run_log -- ---------------------------- DROP TABLE IF EXISTS `job_run_log`; CREATE TABLE `job_run_log` ( `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT, `job_config_id` bigint(11) NOT NULL, `job_name` varchar(64) NOT NULL COMMENT '任务名称', `deploy_mode` varchar(64) NOT NULL COMMENT '提交模式: standalone 、yarn 、yarn-session ', `job_id` varchar(64) DEFAULT NULL COMMENT '运行后的任务id', `local_log` mediumtext COMMENT '启动时本地日志', `remote_log_url` varchar(128) DEFAULT NULL COMMENT '远程日志url的地址', `start_time` datetime DEFAULT NULL COMMENT '启动时间', `end_time` datetime DEFAULT NULL COMMENT '启动时间', `job_status` varchar(32) DEFAULT NULL COMMENT '任务状态', `is_deleted` tinyint(1) NOT NULL DEFAULT '0', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `creator` varchar(32) DEFAULT 'sys', `editor` varchar(32) DEFAULT 'sys', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='运行任务日志'; ALTER TABLE job_run_log add `run_ip` varchar(64) DEFAULT NULL COMMENT '任务运行所在的机器' AFTER local_log ; ALTER TABLE job_run_log ADD COLUMN job_desc VARCHAR(255) NULL COMMENT '任务描述' AFTER job_name; -- ---------------------------- -- Table structure for savepoint_backup -- ---------------------------- DROP TABLE IF EXISTS `savepoint_backup`; CREATE TABLE `savepoint_backup` ( `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT, `job_config_id` bigint(11) NOT NULL, `savepoint_path` varchar(2048) NOT NULL COMMENT '地址', `backup_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '备份时间', `is_deleted` tinyint(1) NOT NULL DEFAULT '0', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `creator` varchar(32) DEFAULT 'sys', `editor` varchar(32) DEFAULT 'sys', PRIMARY KEY (`id`), KEY `index` (`job_config_id`) USING BTREE ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='savepoint备份地址'; -- ---------------------------- -- Table structure for system_config -- ---------------------------- DROP TABLE IF EXISTS `system_config`; CREATE TABLE `system_config` ( `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT, `key` varchar(128) NOT NULL COMMENT 'key值', `val` varchar(512) NOT NULL COMMENT 'value', `type` varchar(12) NOT NULL COMMENT '类型 如:sys alarm', `is_deleted` tinyint(1) NOT NULL DEFAULT '0', `create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `edit_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `creator` varchar(32) NOT NULL DEFAULT 'sys', `editor` varchar(32) NOT NULL DEFAULT 'sys', PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT='系统配置'; -- ---------------------------- -- Table structure for user -- ---------------------------- DROP TABLE IF EXISTS `user`; CREATE TABLE `user` ( `id` int(11) NOT NULL AUTO_INCREMENT, `username` varchar(100) COLLATE utf8mb4_bin NOT NULL COMMENT '用户帐号', `password` varchar(255) COLLATE utf8mb4_bin DEFAULT NULL COMMENT '密码', `status` tinyint(1) NOT NULL COMMENT '1:启用 0: 停用', `is_deleted` tinyint(1) NOT NULL DEFAULT '0' COMMENT '1:删除 0: 未删除', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `creator` varchar(32) COLLATE utf8mb4_bin DEFAULT 'sys', `editor` varchar(32) COLLATE utf8mb4_bin DEFAULT 'sys', PRIMARY KEY (`id`), UNIQUE KEY `index_uk` (`username`) USING BTREE ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin; ALTER TABLE user ADD COLUMN `name` VARCHAR(100) NOT NULL COMMENT '用户姓名' AFTER `username`; CREATE TABLE `job_alarm_config` ( `id` bigint(11) unsigned NOT NULL AUTO_INCREMENT, `job_id` bigint(11) unsigned NOT NULL COMMENT 'job_config主表id', `type` tinyint(1) NOT NULL COMMENT '类型 1:钉钉告警 2:url回调 3:异常自动拉起任务', `version` int(11) NOT NULL DEFAULT '0' COMMENT '更新版本号 ', `is_deleted` tinyint(1) NOT NULL DEFAULT '0', `create_time` datetime DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `edit_time` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `creator` varchar(32) DEFAULT 'sys', `editor` varchar(32) DEFAULT 'sys', PRIMARY KEY (`id`), KEY `uk_index_job_id` (`job_id`) USING BTREE ) ENGINE = InnoDB DEFAULT CHARSET = utf8mb4 COMMENT ='告警辅助配置表'; -- ---------------------------- -- Records of user 默认密码是 123456 -- ---------------------------- BEGIN; INSERT INTO `user` VALUES (1, 'admin', '系统管理员', 'e10adc3949ba59abbe56e057f20f883e', 1, 0, '2020-07-10 22:15:04', '2020-07-24 22:21:35', 'sys', 'sys'); COMMIT; 3.修改数据库连接配置 /flink-streaming-platform-web/conf/application.properties 改成上面建好的mysql地址 4.启动运行 cd /XXXX/flink-streaming-platform-web/bin 启动 : sh deploy.sh start 停止 : sh deploy.sh stop 日志目录地址： /XXXX/flink-streaming-platform-web/logs/ 5.页面配置集群地址 6.官方文档 https://github.com/zhp8341/flink-streaming-platform-web/blob/master/docs/deploy.md © david all right reserved，powered by Gitbook文件修订时间： 2022-05-31 08:26:29 "},"deploy/flink/flink-metrics.html":{"url":"deploy/flink/flink-metrics.html","title":"flink metrics部署","keywords":"","body":"flink metrics部署 1.下载metrics包 wget https://repo1.maven.org/maven2/org/apache/flink/flink-metrics-prometheus/1.14.4/flink-metrics-prometheus-1.14.4.jar 2.配置flink-conf.yaml metrics.reporters: prom metrics.reporter.prom.class: org.apache.flink.metrics.prometheus.PrometheusReporter metrics.reporter.prom.port: 9999 3.重启flink服务 © david all right reserved，powered by Gitbook文件修订时间： 2022-05-31 08:42:09 "}}